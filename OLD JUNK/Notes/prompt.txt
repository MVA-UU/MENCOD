We are working on ASReview, an application designed to find relevant papers related to the paper you have. There is 3 kinds of papers:

- Prior documents => Relevant papers we have before starting our search process.
- Relevant papers => Papers deemed relevant, so prior documents and the documents we are trying to find (those about the same topic as the prior documents)
- Irrelevant papers => All other papers in the database that are about different subjects

Say we have 8000 papers, 26 relevant. This is approximately the dataset we are testing with (called the Appenzeller dataset). ASReview ranks all papers using ML algorithms such as TF-IDF (many choices are possible but this is not important). While 25/26 relevant documents we want to find are ranked very high, there is one document (the outlier) that is for some reason ranked very low. The application works with a stopping rule (which is also configurable) such as "when 100 irrelevant documents are found subsequently, we halt our search". This means that we find the first 25 relevant documents, but never find the last one, as it is located at a way lower rank and ASReview works from top to bottom rank. 

And finding this paper is exactly our task. We are building a new hybrid model, which will be run after the stopping rule is triggered. At that point, we have the 25 relevant at hand and this is valuable information. 

We are trying to formulate a hybrid approach, combining:

- Citation networks
- Scientific embeddings for content similarity (SPECTER2, miniLM, etc.)
- Confidence calibration

We can do everything, so I am always up for suggestions. We should use the best algorithms available, and we can always add more.
First we will build all models standalone. We support multiple datasets for testing as described in the datasets.json file, as our approach should
generalize well to new datasets (performance should remain great). We will rerank the new papers after the initial search process, trying to rank the 
outlier as high as possible in the remaining papers. We should always get top 100 to make it easy to find.

This code will be for my master thesis, so keep in mind we cannot do any cheating or shortcuts. We cannot add code for specific 
datasets, as we want to make one general model that works for all datasets, including unseen ones. We are testing with a couple of 
datasets but these are just for testing purposes, our final aim is not to only have a model that works with these. 

So, make sure:
- you dont add dataset specific code or code that does something different for a specific (type of) dataset
- the same measures should always be applied
- use continuous scaling to make sure we represent our dataset well
- dont use harsh cutoff values or hardcoded values, always dynamically calculate these based on dataset characteristics,
have them be continuous and dont forget to normalize whenever necessary